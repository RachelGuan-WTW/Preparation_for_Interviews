## Situation&Task
实际业务场景中经常会出现或长或短的文字性描述，如何利用这些文字性描述来形成用于lightGBM的特征呢？
## Action
有几下几种方法：
1. 关键词字典：基于关键词字典，利用正则匹配文本描述中出现的关键词

后面的方法都要基于分词。
- 中文分词使用jieba。jieba除了可以自动划分词语，还支持输入自定义字典。</br>
- [snownlp](https://github.com/isnowfy/snownlp)也可以用来中文分词，并且可以判断话语的褒贬程度</br>

2 [词袋模型](https://www.cnblogs.com/HuZihu/p/9576794.html)：也就是统计关键词出现的次数。
- 对于含义相近的词语，只保留一个
- 删除掉无意义的词语
3. [tfidf](https://www.cnblogs.com/HuZihu/p/9576794.html)
4. [N-gram](https://zhuanlan.zhihu.com/p/32829048):反映出词在句子中的含义。 注意要运用词频阈值、tfidf、[停用词](https://github.com/lslstudy/stopwords)等，进行词语筛选，防止维度爆炸。 

后面几种，都要转化词向量word Embedding。</br>
- 词向量word-embedding可以使用[gensim](https://radimrehurek.com/gensim/models/word2vec.html).
```python
    from gensim.test.utils import common_texts
    from gensim.models import Word2Vec
    model = Word2Vec(sentences=common_texts, vector_size=100, window=5, min_count=1, workers=4)
    model.save("word2vec.model")
```
- 词向量还可以使用预训练模型ELMo(Embeddings from Language Models),具体的使用说明见[网址](https://www.analyticsvidhya.com/blog/2019/03/learn-to-use-elmo-to-extract-features-from-text/?utm_source=blog&utm_medium=top-pretrained-models-nlp-article)

5. [sentence2v](https://blog.csdn.net/promisejia/article/details/88364569)总和运用了word2vec加权，PCA去掉共同因素
6. [Bert](https://blog.csdn.net/jiaowoshouzi/article/details/89073944)是Google2018年推出的NLP预训练模型。其中，BERT-Base, Chinese支持中文。</br>

注意：
- 以上Word2vec, ELMo, Bert都属于NLP预训练模型，其余介绍可以参见网址：网上可以找到[史上最详尽的NLP预训练模型汇总](https://zhuanlan.zhihu.com/p/62760251)。</br>
- 以上模型都会涉及上下文。当样本很少，文本较短的情况下，使用该模型效果可能不会特别好

7. 自己使用LSTM+业务Y标签来建模

