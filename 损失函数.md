[toc]



##### 希腊字母

```sql
大写	小写	英文注音	国际音标注音	中文注音
Α	α	alpha	alfa	        阿耳法
Β	β	beta	beta	        贝塔
Γ	γ	gamma	gamma	        伽马
Δ	δ	deta	delta	        德耳塔
Ε	ε	epsilon	epsilon	        艾普西隆
Ζ	ζ	zeta	zeta	        截塔
Η	η	eta	eta	        艾塔
Θ	θ	theta	θita	        西塔
Ι	ι	iota	iota	        约塔
Κ	κ	kappa	kappa	        卡帕
∧	λ	lambda	lambda	        兰姆达
Μ	μ	mu	miu	        缪
Ν	ν	nu	niu	        纽
Ξ	ξ	xi	ksi	        可塞
Ο	ο	omicron	omikron	        奥密可戎
∏	π	pi	pai             派
Ρ	ρ	rho	rou	        柔
∑	σ	sigma	sigma	        西格马
Τ	τ	tau	tau	        套
Υ	υ	upsilon	jupsilon	衣普西隆
Φ	φ	phi	fai	        斐
Χ	χ	chi	khai	        喜
Ψ	ψ	psi	psai	        普西
Ω	ω	omega	omiga	        欧米伽
```

##### 损失函数



1. 0-1损失函数
   $$
   \begin{equation}  
   L(y,\hat{y})=
   \left\{  
                \begin{array}{**lr**}  
                y=1, &  y=\hat{y}\\  
                y=0, & y\not=\hat{y}\\      
                \end{array}  
   \right.  
   \end{equation}
   $$

2. 均方误差损失函数
   $$
   L(y,f(x))=(y-f(x))^2
   $$
   
3. 绝对误差损失函数
   $$
   L(y,f(x))=\left|y-f(x)\right|
   $$
   
4. 对数损失函数
   $$
   L(y,P(y|x))=-log(p(y|x))
   $$
   
5. 交叉熵损失函数
   $$
   L(y,\hat{y})=-y*log(\hat{y})-(1-y)*log(1-\hat{y})
   $$
   Softmax分类器：在神经网络输出值之后，进行转化，再利用交叉熵损失函数
   $$
   p_i={e^{z_i}\over \sum_{j=1}^{k} e^{z_j}}
   $$
   
6. hinge损失函数
   $$
   L(y,f(x))=max(0,1-y*f(x))
   $$

7. 指数损失函数
   $$
   L(y,\hat{y})=exp(-y*\hat{y})
   $$
   
8. 感知机损失函数
   $$
   L(w,b)=- \sum _{i \in M}-y_i*(wx+b)
   $$
   其中M为误分类的样本
   
9. Focalloss损失函数（多分类）
   $$
   FL(p_t) = \sum _{c=1} ^{m} - (1-p_t)^ \gamma *y_c * log(p_t)
   $$
   m表示类别个数，$y_c$是真实标签，$p_t$是预测标签，$\gamma$是调节因子

##### 常用函数

1. sigmoid函数
   $$
   f(x)={1 \over 1+e^{-x}}
   $$

   $$
   f^{'}(x)=(-1)*(1+e^{-x})^{-2}*e^{-x}*(-1)\\
   =f(x)*(1-f(x))
   $$

2. tanh函数
   $$
   tanh(z) = y = {e^z-e^{-z} \over e^z+e^{-z}}
   $$

   $$
   tanh^{'}(z) = 1-y^2
   $$

   

3. relu函数
   $$
   \begin{equation}  
   f(x)=
   \left\{  
                \begin{array}{**lr**}  
                y=x, &  x>=0\\  
                y=0, &  x<0\\      
                \end{array}  
   \right.  
   \end{equation}
   $$
   
4. softmax函数
   $$
   f(z_i)={e^{z_i}\over \sum_{j=1}^{k} e^{z_j}}
   $$
   softmax函数求导

   $i=j$时，求导
   $$
   {e^{z_i}* \sum - e^{z_i}*e^{z_i} \over \sum ^2 }
   $$
   $i\not = j$时 求导
   $$
   {0-e^{z_j}*e^{z_i} \over \sum^2}\\
   = -{e^{z_j} \over \sum}*{e^{z_i} \over \sum} \\
   =-f(z_j)*f(z_i)
   $$
   
5. logistic回归的概率密度函数

   $$
   p(y=1|x)=\hat{y}\\
   p(y=0|x)=1-\hat{y}
   $$
   整合到一起
   $$
   p=\hat{y_i}^{y_i}*(1-\hat{y_i})^{1-y_i}
   $$
   极大似然函数
   $$
   L=\prod_{i=1}^nlog(\hat{y_i}^{y_i}*(1-\hat{y_i})^{1-y_i})
   $$
   取对数
   $$
   log(L)=\sum_{i=1}^n{y_i}*log(\hat{y_i})+(1-y_i)*log(1-\hat{y_i})
   $$




##### 

